---
version: '2'
services:

  # core service required by kafka
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    network_mode: host
    environment:
      ZOOKEEPER_CLIENT_PORT: ${ZOOKEEPER_CLIENT_PORT}
      ZOOKEEPER_TICK_TIME: 2000

  # core service
  kafka:
    image: confluentinc/cp-kafka:latest
    network_mode: host
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: ${KAFKA_ZOOKEEPER_CONNECT}
      KAFKA_ADVERTISED_LISTENERS: ${KAFKA_ADVERTISED_LISTENERS}
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_DELETE_TOPIC_ENABLE: "true"

  # core service provide a rest endpoint to kafka
  kafka-rest:
    image: confluentinc/cp-kafka-rest:3.3.0
    network_mode: host
    depends_on:
      - zookeeper
      - kafka
    environment:
      KAFKA_REST_ZOOKEEPER_CONNECT: ${KAFKA_REST_ZOOKEEPER_CONNECT}
      KAFKA_REST_LISTENERS: ${KAFKA_REST_LISTENERS}
      KAFKA_REST_HOST_NAME: ${KAFKA_REST_HOST_NAME}
      KAFKA_REST_BOOTSTRAP_SERVERS: ${KAFKA_BOOTSTRAP_SERVERS}

  # devops service: provide a ui to examine kafka topics
  ui:
    image: landoop/kafka-topics-ui
    ports:
      - "${KAFKA_TOPIC_UI_PORT}:${KAFKA_TOPIC_UI_PORT}"
    network_mode: host
    environment:
      - KAFKA_REST_PROXY_URL=${KAFKA_REST_LISTENERS}
      - PROXY=true

  # user service: swift instance with our webhook
  swift:
    build:
      context: services/swift
    ports:
      - "${SWIFT_PORT}:${SWIFT_PORT}"
    network_mode: host
    volumes:
      # For testing from host
      - "./volumes/files:/files"
    environment:
      # TODO do env subsitute
      # coordinate with services/swift/files/proxy-server.conf
      - http.host=0.0.0.0
      - KAFKA_TOPIC=${KAFKA_DOS_TOPIC}
      - KAFKA_BOOTSTRAP=${KAFKA_BOOTSTRAP_SERVERS}

  # user service: monitor aws SQS queue
  aws-observer:
    build:
      context: services/aws-observer
    depends_on:
      - zookeeper
      - kafka
    network_mode: host
    environment:
      - KAFKA_TOPIC=${KAFKA_DOS_TOPIC}
      - KAFKA_BOOTSTRAP=${KAFKA_BOOTSTRAP_SERVERS}
      - SQS_QUEUE_NAME=${SQS_QUEUE_NAME}
      - AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION}
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}

  # user service: monitor file system
  file-observer:
    build:
      context: services/file-observer
    network_mode: host
    depends_on:
      - zookeeper
      - kafka
    volumes:
      - "./volumes/files:/files"
    environment:
      - KAFKA_TOPIC=${KAFKA_DOS_TOPIC}
      - KAFKA_BOOTSTRAP=${KAFKA_BOOTSTRAP_SERVERS}
      - MONITOR_DIRECTORY=/files


  # user service: monitor topic, write to elastic
  elastic-sink:
    build:
      context: services/elastic-sink
    network_mode: host
    depends_on:
      - zookeeper
      - kafka
      - elastic
    environment:
      - KAFKA_TOPIC=${KAFKA_DOS_TOPIC}
      - KAFKA_BOOTSTRAP=${KAFKA_BOOTSTRAP_SERVERS}
      - ELASTIC_URL=${ELASTIC_URL}
      

  # Elastic Search
  # note: on docker for mac, you may need to adjust the docker-machines' config
  # if you get this error ....
  # elastic    | ERROR: bootstrap checks failed
  # elastic    | max virtual memory areas vm.max_map_count [65530] likely too low, increase to at least [262144]
  # ... see this
  # https://github.com/elastic/elasticsearch-docker/blob/master/README.md#osx-with-docker-toolbox
  elastic:
    image: docker.elastic.co/elasticsearch/elasticsearch:5.5.3
    volumes:
      - "./volumes/elastic/backups:/backups/"
    ports:
      - "9200:9200"
    network_mode: host
    environment:
      - http.host=0.0.0.0
      - transport.host=127.0.0.1
      - xpack.security.enabled=false
      - path.repo=/backups
